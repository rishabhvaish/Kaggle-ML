---
title: 'Walmart Recruiting II: Sales in Stormy Weather'
author: "Rishabh Vaish"
date: "12/24/2019"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(cache = TRUE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
```

## Introduction 

### Purpose and Goal

The sales of Walmart items are affected by the changes in weather conditions around the stores. The purpose of this project is to predict the sales of 111 weather-sensitive items present in 45 different Walmart stores as they are affected by climate variations. The weather data across 20 different weather stations are provided. We need to predict the sales of items as they are affected by weather events i.e. the days with excessive snowfall and/or rainfall. We predict the sales of items on the day of the weather events, three days before and three days after the weather events. The project focuses on an ordinary least squares linear model. Many improvements are made in the linear model using the concepts taught in the STAT425 course and then they are compared with other more complex models like CatBoost and Bagged Tree.

### Data

Data is provided in 4 tables namely: train, test, weather, and key. The “train” dataset has store number, item number, units sold by date and key for mapping store to nearest weather station from 2012-01-01 to 2014-10-31. The weather table has weather information for all stations for both training and testing period. The key table is used to map from store number to the closest weather station number. The test data contains the dates of weather events on which the predictions are to be made.

### Methodology

The data is pre-processed and the NA values are imputed with the average of the weather information a day before and a day after. Firstly, we removed the item and store combinations that have zero units sold in the training data. These items and store combinations will have zero predictions in the test set. Then the weather events are marked and the data within three days of weather event is picked. Feature engineering is done for this data by adding variables derived from the date like the day of the month,  day of the week, and the quarter of the year. Another column is created which marks whether the date was a holiday or not. The highly correlated features are removed before modeling the remaining non-zero units sold item store combinations ( ~255). A linear model was built using just the weather variables provided and new features engineered. This is considered as the base model and is further improved using techniques like outlier detection and removing influential points. The model is further improved by collinearity analysis and box-cox transformation. Further advanced methods like forward/backward selection with AIC/BIC criteria and LASSO is used to find the best smaller linear model. This best smaller linear model was then compared with the advanced models like CatBoost and Bagged tree.

## Exploratory Data Analysis

The following process was involved in EDA of the dataset, in order -

### Basic

* Many columns of weather data had missing values marked as “M”, these values were replaced by NA
* Some features of weather data were in character data format, they were converted to numeric wherever applicable
* The “Codesum” column was removed from the data

### Missing Values
Since the weather data had a lot of missing values, they were handled using different techniques instead of removing rows. Also, note that “sunrise”, “sunset” and “snowfall” values are missing for all the rows corresponding to station one. Thus they have been handled separately
* Missing values in all the columns like “tmax”, “dewpoint”, “sealevel” etc have been replaced by the average of values a day before and a day after the missing day. This is based on the assumption that weather change occurs in a linear manner instead of random ups and downs
* Special columns like “sunrise”, “sunset” and “snowfall” which have a lot of missing values, have been imputed using the median of the remaining data 

### Feature Engineering
Some new features were derived from the existing features, mainly “date” and added to the weather dataset for further modelling. Following are the features that were added - 
* Day - To incorporate for the day of the week. Eg - “Saturday” and “Sunday” would have higher sales than other days. It was stored as a categorical feature
* Month - To handle the effect of change in sales with seasons. Eg - “December” is expected to have different sales than “July”. It was stored as a categorical feature
* Year - To incorporate the change in sales with yearly trends and inflation. It was also stored as a categorical feature
* Day_month - To incorporate for the change in buying patterns within a month. Eg - People are more likely to buy at the time of salary disbursement. It is a numerical feature
* Week - Another way to incorporate the effect of change in sales with seasons. It is a numerical feature
* Is_holiday - This feature marks the days of national holidays. Eg - People are more probable to shop on the holidays. It is a numeric feature

### Misc
* All the store-item combinations with zero unit sales in the training set were removed and these combinations will be predicted as zero on testing data.
* The weather events are defined as rain > 1 and snowfall >2. Data containing the rows corresponding to weather events and three days before & after the weather event were stored separately as trainset. All the further modelling and processing was done on this data.
* The weather data was then merged with the trainset using the key table. Firstly, the key was added to trainset using a left join in R. Then the weather data with new features was merged with trainset using another left join on trainset in R.
* An ID of the format store_item_date was also added to the dataset for results reporting. 
* Final train data dimensions after EDA is 222,969 * 22

### Graphical Analysis
* Histogram of units sold shows a skew towards the left side. Taking a log of units gives a more normal distribution for dependent variables. This will help in the linear model in further analysis.

* The correlation plot for the initial weather features is shown. Highly correlated features were removed before building a full linear model because they don't add much value to the model.

* A boxplot of all the variables is generated, to visualize the distribution and outliers present in the data. These outliers were later handled for improvement to the linear model


``` {r, include = TRUE}

# Data Load
library(readr)
key <- read_csv("data/key.csv")
test <- read_csv("data/test.csv")
train <- read_csv("data/train.csv")
weather <- read_csv("data/weather.csv")
sample <- read_csv("data/sampleSubmission.csv")

#replace M with NA
library(dplyr)
weather[, 3:ncol(weather)] <- na_if(weather[, 3:ncol(weather)], 'M')

#convert to numeric
char_col <-
  colnames(weather[which(sapply(weather, class) == "character")])
char_col <- char_col[-9]
weather[char_col] <- sapply(weather[char_col], as.numeric)

#handle NA
# order the weather dataframe by station then date
weather <- weather[order(weather$station_nbr, weather$date), ]
#take average of previous and next value
library(zoo)
weather$tmax <-
  (na.locf(weather$tmax) + rev(na.locf(rev(weather$tmax)))) / 2
weather$tmin <-
  (na.locf(weather$tmin) + rev(na.locf(rev(weather$tmin)))) / 2
weather$dewpoint <-
  (na.locf(weather$dewpoint) + rev(na.locf(rev(weather$dewpoint)))) / 2
weather$wetbulb <-
  (na.locf(weather$wetbulb) + rev(na.locf(rev(weather$wetbulb)))) / 2
weather$heat <-
  (na.locf(weather$heat) + rev(na.locf(rev(weather$heat)))) / 2
weather$cool <-
  (na.locf(weather$cool) + rev(na.locf(rev(weather$cool)))) / 2
weather$preciptotal <-
  (na.locf(weather$preciptotal) + rev(na.locf(rev(
    weather$preciptotal
  )))) / 2
weather$stnpressure <-
  (na.locf(weather$stnpressure) + rev(na.locf(rev(
    weather$stnpressure
  )))) / 2
weather$sealevel <-
  (na.locf(weather$sealevel) + rev(na.locf(rev(weather$sealevel)))) / 2
weather$resultspeed <-
  (na.locf(weather$resultspeed) + rev(na.locf(rev(
    weather$resultspeed
  )))) / 2
weather$resultdir <-
  (na.locf(weather$resultdir) + rev(na.locf(rev(weather$resultdir)))) / 2
weather$avgspeed <-
  (na.locf(weather$avgspeed) + rev(na.locf(rev(weather$avgspeed)))) / 2
#recalculate tavg
weather$tavg <- ceiling((weather$tmax + weather$tmin) / 2)

#imputing special cases with median
weather <- tbl_df(data.frame(lapply(weather, function(x) {
  if (is.numeric(x))
    ifelse(is.na(x), median(x, na.rm = T), x)
  else
    x
})))

#handle codesum
weather$codesum <- NULL
summary(weather)

#adding more variables
weather$day <- as.factor(weekdays(weather$date))
weather$month <- as.factor(months(weather$date))
dates <- strptime(as.character(weather$date), format = "%Y-%m-%d")
weather$year <- as.factor(format(dates,'%Y'))
weather$day_month <- as.numeric(format(dates, "%d"))
weather$week <- as.numeric(format(dates, "%V"))
library(tis)
weather$is_holiday <-
  ifelse(isHoliday(
    weather$date,
    goodFriday = F,
    board = T,
    businessOnly = T
  ) == TRUE,
  1,
  0)

#find days of events
event_dates <-
  unique(weather[which(weather$preciptotal > 1 |
                         weather$snowfall > 2), "date"])

event_date_set <- rep(0, nrow(event_dates) * 7)
j = 1
for (i in 1:nrow(event_dates)) {
  event_date_set[j] = event_dates[i, 1] - 3
  j = j + 1
  event_date_set[j] = event_dates[i, 1] - 2
  j = j + 1
  event_date_set[j] = event_dates[i, 1] - 1
  j = j + 1
  event_date_set[j] = event_dates[i, 1]
  j = j + 1
  event_date_set[j] = event_dates[i, 1] + 1
  j = j + 1
  event_date_set[j] = event_dates[i, 1] + 2
  j = j + 1
  event_date_set[j] = event_dates[i, 1] + 3
  j = j + 1
}
event_date_set <- unique(event_date_set)

#weather on event days
weather_event <- weather[which(weather$date %in% event_date_set),]

#Find all the store/items with zero sales
library(dplyr)
units_sold <- train %>%
  group_by(store_nbr, item_nbr) %>%
  summarise(total_sales = sum(units))
#zero units sold
zero_units_sold <-
  units_sold[which(units_sold$total_sales == 0), 1:2]
#atleast one unit sold
units_sold <- units_sold[which(units_sold$total_sales != 0), 1:2]

#graphs
par(mfrow = c(1,2))
hist(train$units)
hist(log(train$units))

#remove useless data
rm(char_col)
rm(event_dates)
rm(event_date_set)
rm(i,j)

```

## Linear Regression

Before creating a full model, we tested the kaggle score with all the predictions for units sold to be zero. The kaggle score for this submission was 0.51, we can consider this as the base score before making a linear model.

### Full Linear Model

In order to create a linear model, we first remove the highly correlated variables in order to remove redundant features. A cut off of 0.75 was used to remove 6 features namely : "wetbulb", "tmin", "tavg", "dewpoint", "tmax" and "avgspeed". After removing these features a linear model named “lm_initial” was created with the following summary -

### Summary
* The residuals had a median of -6.1 and a standard error of 36.66 on 222,931 degrees of freedom. 
* The goodness of fit is determined by R-squared value which came out to be 0.1418, this is not a good fit. Adjusted R-squared is reported to be 0.1416
* The F-statistic is 995.2 which is good and is significant because the p-value is significantly close to zero. Thus we can establish this model is better than a null model. 

### Diagnostics
The diagnostics for the linear model are as follows -

* The residual vs fitted plot shows a linear relationship between residuals and fitted values. The spread across the linear trendline shows that the residuals are not completely uncorrelated.

* The normal QQ plot shows a departure from the linear trendline, thus signifying that the residuals are not normally distributed. A need for transformation is implied.

* The scale location plot is used to check for homoscedasticity. The unequal spread across the trendline shows heteroscedasticity. This is later confirmed using BP Test. 

* Residual vs leverage plot helps us find out the influential points. The plot shows many leverage points. Their influence is later confirmed using a cooks distance measure
  
### Kaggle Score
This model gives a kaggle score of 0.39744


``` {r, include = TRUE}

# using the subset with non zero elements sold for training
# rest will be marked as zero
train_subset <-
  merge(
    x = units_sold,
    y = train,
    by = c("store_nbr", "item_nbr"),
    all.x = TRUE
  )
test_subset <-
  merge(
    x = units_sold,
    y = test,
    by = c("store_nbr", "item_nbr"),
    all.x = TRUE
  )

#find key for train/test subset
train_subset_key <-
  merge(x = train_subset,
        y = key,
        by = "store_nbr",
        all.x = TRUE)
test_subset_key <-
  merge(x = test_subset,
        y = key,
        by = "store_nbr",
        all.x = TRUE)

#adding weather data to test/train subset
trainset <-
  merge(x = train_subset_key,
        y = weather_event,
        by = c("station_nbr", "date"))
testset <-
  merge(x = test_subset_key,
        y = weather_event,
        by = c("station_nbr", "date"))
#creating an ID for reference
testset$id <-
  paste(testset$store_nbr,
        "_",
        testset$item_nbr,
        "_",
        testset$date,
        sep = "")

#shifting units to last column
trainset$unit_temp <- trainset$units
trainset$units <- NULL
trainset$units <- trainset$unit_temp
trainset$unit_temp <- NULL

#checking correlation before lm
library(caret)
#check correlation between numeric variables
trainset_num <-
  trainset[which(sapply(trainset, class) == "numeric")]
testset_num <- testset[which(sapply(testset, class) == "numeric")]
descrCor <-  cor(trainset_num)
#high correlation
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
colnames(trainset_num[, highlyCorDescr])
#remove highly correlated data
trainset$wetbulb <- NULL
trainset$dewpoint <- NULL
trainset$tmin <- NULL
trainset$tavg <- NULL
trainset$tmax <- NULL
trainset$avgspeed <- NULL

#some graphs
library(reshape2)
melted_cormat <- melt(descrCor)
head(melted_cormat)
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
#boxplot of data
box_data <- weather[, -2]
box_data$sunrise <- NULL
box_data$sunset <- NULL
boxplot(box_data, main = "Boxplot of variables")

# Simple OLS
lm_initial <- lm(units ~ . , data = trainset)
summary(lm_initial)
plot(lm_initial)

#predictions using OLS for Kaggle score
testset$pred <- predict(lm_initial, newdata = testset)
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)

write.csv(sample, "submit.csv", row.names = FALSE)
#kaggle score = 0.39744

#remove extra things
rm(dates, descrCor, test_subset, test_subset_key, testset_num)
rm(train_subset, train_subset_key, trainset_num)

```

## Improvements

The kaggle score and R-squared are used as a metric for judging the improvement. The kaggle score is reported at each step of improvement. The following steps were taken to improve the full linear model -

### Outliers and Influential Points
* Outliers are points that deviate from general trend followed by the data. Such points can lead to decrease in overall model performance and hence need to be dealt with.
* The residual studentized values were calculated using the full model and all the points with studentized residual greater than 2 i.e. the residual is more than 2 standard deviations away from the mean were removed. 
* This method removed around 8535 rows and the kaggle score was improved to 0.37250

* Influential points also need to be removed from the data since they can have a negative impact on the overall model. Cook’s distance is a measure to find influential points. It is directly proportional to the difference in predicted value for an observation when the model is fitted with/without that particular observation. 
* Since the residual vs leverage plot showed a lot of leverage points, we checked the cooks distance for each and every point and used 4/n (~0.00002) as the cut off for removing influential points, where n is the total no. of observations.
* This technique further removed 428 rows and improved the kaggle score to 0.370

### Collinearity
* Collinearity is a condition when one or more variables are linearly dependant on some other variables in the data. * Collinearity can lead to problems like not getting the optimal solution and the non-identifiability of the linear model.
* In order to check for collinearity in variables, we calculated the Variance Inflation factors for the full model. * A VIF value greater than 10 is considered to be a multicollinearity issue. 
* The initial check showed three variables with high VIF namely: Date, year and Day_month. We removed “date” and rebuild a model to recheck the VIF scores. This time all the VIF scores were below 5. 
* By removing the “date” column we were able to handle the issue of collinearity and rechecked the kaggle score to be consistent (~0.371). This was expected because we just removed a variable collinear with other variables and removing it didn’t actually remove any information from the model.

### Homoscedasticity
* A BP test was done to check for the assumption of constant variance. The null hypothesis of homoscedasticity was rejected because the test produced a P-value of 1.214e-09. 
* Thus, we need a transformation to tackle heteroscedasticity and non-normality among the residuals.

### Transformations
* We implemented a box-cox transformation on the initial full model. Box cox transformation is known for reducing non-normality of errors as well as heteroscedasticity. It involves raising the target y to a power lambda and then obtain a lambda estimate such that likelihood of data obtained is maximised. The lambda value obtained is generally rounded off to the nearest integer. For lambda=0, log transformation is performed.
* The lambda value for maximum likelihood was achieved between -0.5 and 0.
* We selected 0 as the optimal lambda and took a logarithm of units plus one sold item as the dependent variable. 
* We then took the predictions of this model and subtracted one to achieve our final prediction after transformation. 
* The kaggle score improved significantly to 0.32593

### Best Subset Selection
* In order to select a smaller model with similar or better prediction than the full model, we implemented various strategies like forward/ backward selection with AIC/BIC criteria. An ANOVA F-test was done to check if the new smaller model is better than the full model. Following were the results achieved -

* AIC Backward - This gave the same model as the full model, showing none of the smaller models is better.
* BIC Backwards - Since BIC has a higher penalty than AIC, so it selects a smaller model with just three features less than the full model namely: “snowfall”, “preciptotal”  and “stnpressure”.  An ANOVA F-test on this model shows a p-value smaller than 0.05 thus we reject the null hypothesis and conclude that the smaller model is not better than the full model
* AIC Forward - This gave a smaller model with features like “stnpressure” removed. An ANOVA F-test on this model gave P-value > 0.05, thus we can establish that the smaller model is equivalent to the larger model. The kaggle score for this model is 0.326
* BIC Forward - The model was significantly smaller than the full model. The ANOVA F-test shows a high P-value thus this model is not equivalent to the full model. We can discard this model.
* Using AIC/BIC criteria, AIC Forward stepwise selection gives the best model, with “stnpressure” removed from the full model. We tried a stepwise selection with different criteria, but the improvement in results is not significant. Thus we can conclude that the subset of these features is not producing a good enough or comparable model.

### LASSO
Another way of selecting variables for a linear model is to use LASSO based linear regression. LASSO adds l1 penalty to the loss function which is proportional to the coefficients and, thus shrinks the coefficients leading to less overfitting. Generally in this technique coefficients of some variables which are not important are reduced to zero. This is because of the intersection of constraint boundaries with the cost function contours at specific points. So we implemented LASSO using cv.glmnet and the coefficients of “snowfall” and “preciptotal” reduced to zero. Training a linear model with remaining features resulted in a kaggle score of 0.326.

### Best Linear Model
The best linear model after all the improvements was the one after implementing box-cox transformation which gave a Kaggle score 0.32593. Here we are discussing the summary and diagnostics of the best linear model as compared with the full model. Thus showing the overall improvements 

### Summary
* The residuals had a median of -0.2 and a standard error of 1.367 which is a good improvement as compared to -6.1 and 36.66 of the full model 
* The goodness of fit determined by R-squared value which came out to be 0.1418 for the full linear model, has increased to 0.304. 
* The F-statistic has also improved significantly from 995.2 for the full model to 2598 for the best model. Both the P-values show that the F-statistic is significant and hence the model is useful 

### Diagnostics
* The residual vs fitted plot still show a linear relationship between residuals and fitted values. Although, the spread across the linear trendline shows that the residuals decrease with increasing fitted values. Hence the residuals are not uncorrelated.

* The normal QQ plot has improved from earlier but it still shows a departure from the linear trendline towards the ends, thus signifying that the residuals are not normally distributed.

* The unequal spread across the trendline shows heteroscedasticity

* Some leverage points are present but all the influential points are removed.


``` {r, include = TRUE}

#check for outliers
trainset$student.res = rstandard(lm_initial)
trainset_new <- trainset[which(abs(trainset$student.res) <= 2), ]
trainset_new$student.res <- NULL
trainset$student.res <- NULL

#fit a model without outliers
lm_second <- lm(units ~ . , data = trainset_new)
summary(lm_second)

#plot the graph after outliers removal
par(mforw = c(1, 2))
plot(rstudent(lm_initial), main = "Full data")
plot(rstudent(lm_second), main = "Removed outliers")

#make predictions after outliers removal
testset$pred <- predict(lm_second, newdata = testset)
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit2.csv", row.names = FALSE)
# kaggle score = 0.37250

#check for influential points using 4/n as cut off
trainset$cooks = cooks.distance(lm_initial)
trainset$student.res = rstandard(lm_initial)
trainset_new <-
  trainset[which(abs(trainset$cooks) <= 0.00002 &
                   abs(trainset$student.res) <= 2), ]
trainset_new$cooks <- NULL
trainset$cooks <- NULL
trainset_new$student.res <- NULL
trainset$student.res <- NULL

#fit a model without influential points
lm_third <- lm(units ~ . , data = trainset_new)
summary(lm_third)

#plot the graph after influential point removal
par(mforw = c(1, 2))
plot(cooks.distance(lm_initial), type = "h", main = "Influential points")
plot(cooks.distance(lm_third), type = "h", main = "Influential points removed")

#make predictions after influential point removal
testset$pred <- predict(lm_third, newdata = testset)
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit3.csv", row.names = FALSE)
# kaggle score = 0.37032

library(car)
#check for collinearity
vif(lm_initial)

#remove one variable
trainset_new$date <- NULL

#fit a model
lm4 <- lm(units ~ . , data = trainset_new)

#make predictions after collinearity removal
testset$pred <- predict(lm4, newdata = testset)
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit4.csv", row.names = FALSE)
# kaggle score = 0.37099

#check for collinearity again
vif(lm4)

#checking homoscedasticity
library(lmtest)
bptest(lm_initial)
#not homoscedasticity

#check for transformations
#boxcox
library(MASS)
boxcox(lm(units + 1 ~ ., data = trainset))
#since Lambda is closer to 0
# i take log transformations

#create a model with transformation
lm5 <- lm(log1p(units) ~ . , data = trainset_new)
summary(lm5)

#make predictions for the model with transformations
testset$pred <- exp(predict(lm5, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit5.csv", row.names = FALSE)
# kaggle = 0.32593

#selecting best subset model
lm6 <- lm5
#AIC Backwards
aic_backward = step(lm6, trace = 0)
aic_backward

testset$pred <- exp(predict(aic_backward, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit7.csv", row.names = FALSE)
#kaggle score = 0.32593

#BIC backwards
bic_backward = step(lm6, trace = 0, k = log(nrow(trainset_new)))
bic_backward

testset$pred <- exp(predict(bic_backward, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit8.csv", row.names = FALSE)
# kaggle score = 0.32608

model_intercept = lm(log1p(units) ~ 1, data = trainset_new)
#aic forward
aic_forward = step(
  model_intercept,
  scope = list(upper = lm6),
  trace = 0,
  direction = "forward"
)
aic_forward

testset$pred <- exp(predict(aic_forward, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit9.csv", row.names = FALSE)
#kaggle score = 0.32593

#bic forward
bic_forward = step(
  model_intercept,
  scope = list(upper = lm6),
  trace = 0,
  direction = "forward",
  k = log(nrow(trainset_new))
)
bic_forward

testset$pred <- exp(predict(bic_forward, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit10.csv", row.names = FALSE)
#kaggle score = 0.32477

anova(aic_backward, lm6) # good but same as before
anova(bic_backward, lm6) # not good
anova(aic_forward, lm6) # good and one variable less
anova(bic_forward, lm6) # not good

#using lasso for best subset selection
x <- model.matrix(lm6)
x <- x[, -1]
y <- log1p(trainset_new[[ncol(trainset_new)]])
library(glmnet)
cvfit <- glmnet::cv.glmnet(x, y, alpha = 1)
coef(cvfit, s = "lambda.1se")

#removing snowfall and preciptotal
trainset_new$snowfall <- NULL
trainset_new$preciptotal <- NULL

#make a new moel for lasso
lm11 <- lm(log1p(units) ~ . , data = trainset_new)

#make predictions
testset$pred <- exp(predict(lm11, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id), "pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample, "submit11.csv", row.names = FALSE)
# kaggle score = 0.32607

```

## Extra Models

We have explored the linear relationship between all our children’s predictor variables and the sales variable. Now we’d like to check for non-linear relationships between them and tree-based models are perfectly suited for the same. 

In a single tree model, we start with all the observations and start splitting them based of different features, subsequently building a top-down tree, with the leaves at the bottom representing the final outcome of the model. At the leaf level, the observations under each bucket are averaged to get the final outcome. 

Tree-based models are largely averting to outliers and missing values. Multicollinearity is also not an issue. At the same time, they are very easy to interpret and often lead to extremely good accuracy scores. We have explored 2 tree-based models, namely CatBoost and BaggedTree for this data.

### Cross-validation
Cross-validation is the technique of dividing the training data into two subsets, training and validation, and evaluating the performance of the model by only training on the former subset generated. We have used 5-fold cross-validation for both the extra models, hence dividing the data into 5 equal chunks. Each time the model is only 4 chunks out of these 5 and is evaluated on the remaining chunk. This step is repeated 5 times and the final RMSE score is the average of all the 5 models. The objective of doing this is that every observation gets to be a part of the validation set exactly once. 

### CatBoost
CatBoost is a type of gradient boosting algorithm which uses trees as weak learners and works well when there are many categorical features having a significant number of values. Boosting is a method which involved combining weak learners to form a strong one. In the beginning, the base learner assigned equal weight age to all observations. Many such base models are added sequentially. In the subsequent models, the gradient of the previous models is optimized. This is how later base models pay more attention to rows where the previous models failed to predict correctly.  Eventually, all these weak learners are combined and a strong learner is constructed.
The most important important reason for choosing CatBoost is its ability to deal with categorical features out of the box without the need for any preprocessing. We have 3 categorical variables in our data, namely “year”, “month” and “day”. Also it deals with missing values automatically. CatBoost is also able to generate combinations of categorical features, thus forming additional categorical features of higher-order.

The best Kaggle score achieved is  0.099. The hyperparameters obtained using Gridsearch are as follows:
iterations - 1000
depth of tree - 10
Learning_rate = 0.1
Rsm (random subspace method) = 0.95 

### RandomForest
RandomForest is a completely different type of ensemble method, which relies on the concept of bagging. From the original data, N samples with replacement are generated for N trees which form the complete forest. On average, each sample contains ~66.6% of the rows from the original data. The idea is to show a different subset of the data to each individual decision tree to introduce randomness in the model. At the same time, each tree only gets to split on a subset of the total features, which is governed by a hyperparameter. A single decision tree which is unpruned is bound to overfit the data, which is not the case with RandomForest. The predictions of all the trees in the model are averaged to obtain the final predictions. 

The hyperparameters used for training RandomForest are
Mtry - 4
Nodesize - 5
Ntree - 500
Results

``` {r, include = TRUE}

#Catboost

library(caret)
library(catboost)

#make data format
xtrain <- trainset_new[,-ncol(trainset_new)]
ytrain <- log1p(trainset_new$units)
#categorical variables
cat_var <- which(sapply(xtrain, class) == "factor")
#pool data
train_pool <-
  catboost.load_pool(data = xtrain,
                     label = ytrain,
                     cat_features = cat_var)
#set params
params <- list(
  iterations = 1000,
  learning_rate = 0.1,
  depth = 10,
  loss_function = 'RMSE',
  eval_metric = 'RMSE',
  random_seed = 208,
  od_type = 'Iter',
  metric_period = 50,
  od_wait = 20,
  rsm = 0.95
)
#train the model
model_text <-
  catboost.train(learn_pool = train_pool,
                 params = params)
#create testing data
xtest <- testset
xtest$date <- as.numeric(xtest$date)
xtest$id <- as.factor(xtest$id)
#create pool
test_pool <-
  catboost.load_pool(data = xtest,
                     cat_features = cat_var)
#make prediction
testset$pred <- exp(catboost.predict(model_text, pool = test_pool)) - 1
sample$units <- testset[match(sample$id, testset$id),"pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample,"submit12.csv", row.names = FALSE)
#Kaggle score = 0.9949

#########################################################
# Random Forest
#########################################################
library(doParallel)
cl <- makePSOCKcluster(12)
registerDoParallel(cl)

start_time <- Sys.time()
ctrl <- trainControl(method = "cv", number = 3, allowParallel=TRUE)
rf_fit <- train(log1p(units) ~ ., data = trainset, method="ranger", trControl = ctrl )
end_time <- Sys.time()
#time taken
end_time - start_time

stopCluster(cl)

rf_fit

testset$pred <- exp(predict(rf_fit, newdata = testset)) - 1
sample$units <- testset[match(sample$id, testset$id),"pred"]
sample$units[is.na(sample$units)] <- 0
sample$units <- ifelse(sample$units <= 0 , 0, sample$units)
write.csv(sample,"submit13.csv", row.names = FALSE)
#Kaggle score = 0.09890

```

The Kaggle scores of all the four methods shown here - 

* Full linear Model = 0.39744
* Best linear Model = 0.32593
* CatBoost = 0.0994
* Random Forest = 0.09890


